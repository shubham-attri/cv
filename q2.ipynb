{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":10787233,"sourceType":"datasetVersion","datasetId":6694201}],"dockerImageVersionId":30887,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"%pip install -q wandb python-dotenv pandas scikit-learn torch torchvision Pillow zipfile36 matplotlib\n\n\nimport os\nfrom dotenv import load_dotenv\nimport zipfile\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms\nfrom torchvision.io import read_image\nimport torch\nfrom torchvision import transforms\nfrom PIL import Image\nimport numpy as np \nimport pandas as pd\nimport wandb\n\n!warnings.filterwarnings(\"ignore\")\n\n\n\nload_dotenv()\n# wandb = os.getenv('wanddb')\n\n!wandb login --verify db5d814ded3a1623b2784d7680a210d6233f7432\n\nZIP_PATH = 'Cropped_final.zip'\nFOLDER_PATH = '/kaggle/input/cva1q2/Cropped_final'\nBATCH_SIZE = 32\nIMAGE_SIZE = 224\nWANDB_PROJECT = \"Wildflife Project\"\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-19T12:38:04.137087Z","iopub.execute_input":"2025-02-19T12:38:04.137323Z","iopub.status.idle":"2025-02-19T12:38:22.482954Z","shell.execute_reply.started":"2025-02-19T12:38:04.137302Z","shell.execute_reply":"2025-02-19T12:38:22.482012Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Function to extract the dataset\ndef extract_dataset(zip_path=ZIP_PATH,log=False):\n    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n        zip_ref.extractall()\n        if log: \n            print(\"Dataset extracted successfully!\")\n\n# Need to do when we have zip and need to extract\n# extract_dataset() \n\n\nlabel_mapping = {\n    'amur_leopard': 0,\n    'amur_tiger': 1,\n    'birds': 2,\n    'black_bear': 3,\n    'brown_bear': 4,\n    'dog': 5,\n    'roe_deer': 6,\n    'sika_deer': 7,\n    'wild_boar': 8,\n    'people': 9\n}\n\nimage_paths = []\nlabels = []\n\nfor folder in os.listdir(FOLDER_PATH):\n    folder_path = os.path.join(FOLDER_PATH, folder)\n    if os.path.isdir(folder_path) and folder in label_mapping:\n        for file in os.listdir(folder_path):\n            file_path = os.path.join(folder_path, file)\n            image_paths.append(file_path)\n            labels.append(label_mapping[folder])\n            \ndf = pd.DataFrame({\n    \"image_path\": image_paths,\n    \"label\": labels\n})\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-19T12:38:22.484104Z","iopub.execute_input":"2025-02-19T12:38:22.484467Z","iopub.status.idle":"2025-02-19T12:38:23.518676Z","shell.execute_reply.started":"2025-02-19T12:38:22.484439Z","shell.execute_reply":"2025-02-19T12:38:23.517741Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Split into 80 20\ntrain_df, val_df = train_test_split(df, test_size=0.2, random_state=55, stratify=df['label'])\nprint(f\"Train set: {len(train_df)} samples, Validation set: {len(val_df)} samples\")\n\n\n# Define transforms with resize\ntransform = transforms.Compose([\n    transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),  # Resize all images to same dimensions\n    transforms.ToTensor(),  # Convert to tensor\n    \n])\n\nclass CustomImageDataset(Dataset):\n    def __init__(self, dataframe, transform=None):\n        self.dataframe = dataframe.reset_index(drop=True)\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.dataframe)\n\n    def __getitem__(self, idx):\n        row = self.dataframe.iloc[idx]\n        image_path = row['image_path']\n        label = row['label']\n        image = Image.open(image_path).convert(\"RGB\")\n        \n        # Apply transforms\n        if self.transform:\n            image = self.transform(image)\n            \n        return image, label\n\n# Create datasets with the transforms\ntrain_dataset = CustomImageDataset(train_df, transform=transform)\nval_dataset = CustomImageDataset(val_df, transform=transform)\n\n# Create data loaders\ntrain_loader = DataLoader(\n    train_dataset, \n    batch_size=BATCH_SIZE, \n    shuffle=True, \n    num_workers=2\n)\n\nval_loader = DataLoader(\n    val_dataset, \n    batch_size=BATCH_SIZE, \n    shuffle=False, \n    num_workers=2\n)\n\n\n# Initialize Weights & Biases (WandB)\nwandb.init(project=WANDB_PROJECT, name=\"stratified-split\")\nwandb.config.batch_size = BATCH_SIZE\n\n# Get label names from label_mapping (assuming you have this dictionary)\nlabel_names = {v: k for k, v in label_mapping.items()}  # Reverse the mapping\n\n# Create figure\nfig, axs = plt.subplots(1, 2, figsize=(15, 6))\n\n# Compute the counts of each label in both splits\ntrain_counts = train_df['label'].value_counts().sort_index()\nval_counts = val_df['label'].value_counts().sort_index()\n\n# Plot training set distribution\naxs[0].bar(range(len(train_counts)), train_counts.values, color='blue')\naxs[0].set_title(\"Train Set Label Distribution\")\naxs[0].set_xlabel(\"Class\")\naxs[0].set_ylabel(\"Count\")\naxs[0].set_xticks(range(len(train_counts)))\naxs[0].set_xticklabels([label_names[i] for i in train_counts.index], rotation=45, ha='right')\n\n# Plot validation set distribution\naxs[1].bar(range(len(val_counts)), val_counts.values, color='green')\naxs[1].set_title(\"Validation Set Label Distribution\")\naxs[1].set_xlabel(\"Class\")\naxs[1].set_ylabel(\"Count\")\naxs[1].set_xticks(range(len(val_counts)))\naxs[1].set_xticklabels([label_names[i] for i in val_counts.index], rotation=45, ha='right')\n\n# Adjust layout to prevent label cutoff\nplt.tight_layout()\nplt.show()\n\n# Log the distribution plot to WandB\nwandb.log({\"Label Distribution\": wandb.Image(fig, caption=\"Train and Validation Label Distribution\")})\nplt.close(fig)\nwandb.finish()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-19T12:38:23.519549Z","iopub.execute_input":"2025-02-19T12:38:23.519802Z","iopub.status.idle":"2025-02-19T12:38:38.172187Z","shell.execute_reply.started":"2025-02-19T12:38:23.519780Z","shell.execute_reply":"2025-02-19T12:38:38.171568Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch.nn as nn\n\nclass CustomCNN(nn.Module):\n    def __init__(self, num_classes=10):  # 10 classes as per your label_mapping\n        super(CustomCNN, self).__init__()\n        \n        # First Convolutional Block\n        self.conv1 = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, stride=1, padding=1)\n        self.relu1 = nn.ReLU()\n        self.pool1 = nn.MaxPool2d(kernel_size=4, stride=4)\n        \n        # Second Convolutional Block\n        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1)\n        self.relu2 = nn.ReLU()\n        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n        \n        # Third Convolutional Block\n        self.conv3 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1)\n        self.relu3 = nn.ReLU()\n        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n        \n        # Calculate the size of flattened features\n        # Assuming input image size is 224x224 (standard size)\n        # After pool1 (4,4): 56x56\n        # After pool2 (2,2): 28x28\n        # After pool3 (2,2): 14x14\n        self.flatten_size = 128 * 14 * 14\n        \n        # Classification head\n        self.classifier = nn.Sequential(\n            nn.Linear(self.flatten_size, 512),\n            nn.ReLU(),\n            nn.Dropout(0.5),\n            nn.Linear(512, num_classes)\n        )\n        \n    def forward(self, x):\n        # First block\n        x = self.conv1(x)\n        x = self.relu1(x)\n        x = self.pool1(x)\n        \n        # Second block\n        x = self.conv2(x)\n        x = self.relu2(x)\n        x = self.pool2(x)\n        \n        # Third block\n        x = self.conv3(x)\n        x = self.relu3(x)\n        x = self.pool3(x)\n        \n        # Flatten\n        x = x.view(x.size(0), -1)\n        \n        # Classification head\n        x = self.classifier(x)\n        return x\n\n# Initialize the model\nmodel = CustomCNN()\n\n# If you have a GPU available, move the model to GPU\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = model.to(device)\n\n# Define loss function and optimizer\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-19T12:38:38.173041Z","iopub.execute_input":"2025-02-19T12:38:38.173334Z","iopub.status.idle":"2025-02-19T12:38:38.617394Z","shell.execute_reply.started":"2025-02-19T12:38:38.173304Z","shell.execute_reply":"2025-02-19T12:38:38.616546Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# import wandb\n\n# Initialize wandb\nwandb.init(project=WANDB_PROJECT, name=\"cnn-training\")\nwandb.config.update({\n    \"batch_size\": BATCH_SIZE,\n    \"epochs\": 10,\n    \"learning_rate\": optimizer.param_groups[0]['lr'],\n    \"model\": \"CustomCNN\"\n})\n\ndef train_epoch(model, train_loader, criterion, optimizer, device):\n    model.train()\n    running_loss = 0.0\n    correct = 0\n    total = 0\n    \n    for images, labels in train_loader:\n        images, labels = images.to(device), labels.to(device)\n        \n        optimizer.zero_grad()\n        outputs = model(images)\n        loss = criterion(outputs, labels)\n        \n        loss.backward()\n        optimizer.step()\n        \n        running_loss += loss.item()\n        _, predicted = outputs.max(1)\n        total += labels.size(0)\n        correct += predicted.eq(labels).sum().item()\n    \n    epoch_loss = running_loss / len(train_loader)\n    epoch_acc = 100. * correct / total\n    return epoch_loss, epoch_acc\n\ndef validate(model, val_loader, criterion, device):\n    model.eval()\n    running_loss = 0.0\n    correct = 0\n    total = 0\n    \n    with torch.no_grad():\n        for images, labels in val_loader:\n            images, labels = images.to(device), labels.to(device)\n            \n            outputs = model(images)\n            loss = criterion(outputs, labels)\n            \n            running_loss += loss.item()\n            _, predicted = outputs.max(1)\n            total += labels.size(0)\n            correct += predicted.eq(labels).sum().item()\n    \n    val_loss = running_loss / len(val_loader)\n    val_acc = 100. * correct / total\n    return val_loss, val_acc\n\n# Training loop\nnum_epochs = 10\nbest_val_acc = 0.0\n\nfor epoch in range(num_epochs):\n    # Train\n    train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)\n    \n    # Validate\n    val_loss, val_acc = validate(model, val_loader, criterion, device)\n    \n    # Log metrics to wandb\n    wandb.log({\n        \"epoch\": epoch + 1,\n        \"train_loss\": train_loss,\n        \"train_accuracy\": train_acc,\n        \"val_loss\": val_loss,\n        \"val_accuracy\": val_acc\n    })\n    \n    # Print metrics\n    print(f'Epoch [{epoch+1}/{num_epochs}]')\n    print(f'Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%')\n    print(f'Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%')\n    print('-' * 50)\n    \n    \n    torch.save(model.state_dict(), 'CustomCNN.pth')\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-19T12:38:38.619748Z","iopub.execute_input":"2025-02-19T12:38:38.619969Z","iopub.status.idle":"2025-02-19T12:48:48.362570Z","shell.execute_reply.started":"2025-02-19T12:38:38.619950Z","shell.execute_reply":"2025-02-19T12:48:48.361473Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nfrom sklearn.metrics import accuracy_score, f1_score, confusion_matrix\nimport seaborn as sns\n\ndef evaluate_model(model, val_loader, device):\n    model.eval()\n    all_preds = []\n    all_labels = []\n    \n    with torch.no_grad():\n        for images, labels in val_loader:\n            images, labels = images.to(device), labels.to(device)\n            outputs = model(images)\n            _, predicted = outputs.max(1)\n            \n            all_preds.extend(predicted.cpu().numpy())\n            all_labels.extend(labels.cpu().numpy())\n    \n    # Calculate metrics\n    accuracy = accuracy_score(all_labels, all_preds)\n    f1 = f1_score(all_labels, all_preds, average='weighted')\n    conf_matrix = confusion_matrix(all_labels, all_preds)\n    \n    # Create confusion matrix plot\n    plt.figure(figsize=(10, 8))\n    sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues')\n    plt.title('Confusion Matrix')\n    plt.xlabel('Predicted')\n    plt.ylabel('True')\n    plt.show()\n    \n    # Log metrics to wandb\n    wandb.log({\n        \"Validation Accuracy\": accuracy,\n        \"Validation F1 Score\": f1,\n        \"Confusion Matrix\": wandb.Image(plt)\n    })\n    \n    print(f\"Validation Accuracy: {accuracy:.4f}\")\n    print(f\"Validation F1 Score: {f1:.4f}\")\n    \n    plt.close()\n\n# Run evaluation\nevaluate_model(model, val_loader, device)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-19T12:48:48.364700Z","iopub.execute_input":"2025-02-19T12:48:48.364958Z","iopub.status.idle":"2025-02-19T12:48:59.588810Z","shell.execute_reply.started":"2025-02-19T12:48:48.364933Z","shell.execute_reply":"2025-02-19T12:48:59.588074Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport numpy as np\nimport torch\nimport wandb\n\ndef get_misclassified_samples(model, val_loader, device, num_samples=3):\n    model.eval()\n    misclassified = {i: [] for i in range(10)}  # Dictionary for each class\n    \n    with torch.no_grad():\n        for images, labels in val_loader:\n            images, labels = images.to(device), labels.to(device)\n            outputs = model(images)\n            _, predicted = outputs.max(1)\n            \n            # Find misclassified samples\n            mask = predicted != labels\n            misclassified_indices = mask.nonzero().squeeze()\n            \n            for idx in misclassified_indices:\n                true_label = labels[idx].item()\n                pred_label = predicted[idx].item()\n                \n                # Store only if we need more samples for this class\n                if len(misclassified[true_label]) < num_samples:\n                    misclassified[true_label].append({\n                        'image': images[idx].cpu(),\n                        'true_label': true_label,\n                        'pred_label': pred_label\n                    })\n    \n    return misclassified\n\n# Get class names (reverse of label_mapping)\nclass_names = {v: k for k, v in label_mapping.items()}\n\n# Get misclassified samples\nmisclassified_samples = get_misclassified_samples(model, val_loader, device)\n\n# Visualize misclassified samples for each class\nfig = plt.figure(figsize=(15, 25))\nfor class_idx in range(10):\n    samples = misclassified_samples[class_idx]\n    \n    for i, sample in enumerate(samples[:3]):\n        plt.subplot(10, 3, class_idx * 3 + i + 1)\n        # Convert tensor to image\n        img = sample['image'].permute(1, 2, 0)  # Change from CxHxW to HxWxC\n        plt.imshow(img)\n        plt.title(f'True: {class_names[sample[\"true_label\"]]}\\nPred: {class_names[sample[\"pred_label\"]]}')\n        plt.axis('off')\n\nplt.tight_layout()\n\n# Log the figure to wandb\nwandb.log({\"Misclassified Samples\": wandb.Image(fig)})\nplt.show()\nplt.close()\n\n# Analyze and log confusion patterns\nconfusion_analysis = \"\"\nfor class_idx, samples in misclassified_samples.items():\n    if samples:\n        confusion_analysis += f\"\\nClass: {class_names[class_idx]}\\n\"\n        pred_counts = {}\n        for sample in samples:\n            pred_label = sample['pred_label']\n            pred_class = class_names[pred_label]\n            pred_counts[pred_class] = pred_counts.get(pred_class, 0) + 1\n        \n        confusion_analysis += \"Misclassified as:\\n\"\n        for pred_class, count in pred_counts.items():\n            confusion_analysis += f\"- {pred_class}: {count} times\\n\"\n\n# Print and log confusion analysis\nprint(\"\\nConfusion Analysis:\")\nprint(confusion_analysis)\n\n# Log confusion analysis to wandb as a text file\nwandb.log({\"confusion_analysis\": wandb.Html(f\"<pre>{confusion_analysis}</pre>\")})\nwandb.finish()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-19T12:48:59.589941Z","iopub.execute_input":"2025-02-19T12:48:59.590473Z","iopub.status.idle":"2025-02-19T12:49:15.180710Z","shell.execute_reply.started":"2025-02-19T12:48:59.590446Z","shell.execute_reply":"2025-02-19T12:49:15.179793Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torchvision.models as models\n\nclass FineTunedResNet18(nn.Module):\n    def __init__(self, num_classes=10):\n        super(FineTunedResNet18, self).__init__()\n        # Load pretrained ResNet-18\n        self.model = models.resnet18(pretrained=True)\n        \n        # Freeze all layers initially\n        for param in self.model.parameters():\n            param.requires_grad = False\n            \n        # Replace the final layer\n        num_features = self.model.fc.in_features\n        self.model.fc = nn.Linear(num_features, num_classes)\n        \n        # Store the backbone (everything except the final layer)\n        self.backbone = nn.Sequential(*list(self.model.children())[:-1])\n    \n    def forward(self, x):\n        return self.model(x)\n    \n    def extract_features(self, x):\n        # Get features before the final classification layer\n        features = self.backbone(x)\n        return features.view(features.size(0), -1)\n\n# Initialize model, criterion, and optimizer\nmodel = FineTunedResNet18(num_classes=10).to(device)\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters())\n\n# Initialize wandb\nwandb.init(project=WANDB_PROJECT, name=\"resnet18-finetuning\")\nwandb.config.update({\n    \"model\": \"ResNet18\",\n    \"epochs\": 10,\n    \"batch_size\": BATCH_SIZE,\n    \"learning_rate\": optimizer.param_groups[0]['lr']\n})","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-19T12:49:15.181832Z","iopub.execute_input":"2025-02-19T12:49:15.182118Z","iopub.status.idle":"2025-02-19T12:49:15.803792Z","shell.execute_reply.started":"2025-02-19T12:49:15.182096Z","shell.execute_reply":"2025-02-19T12:49:15.802915Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def train_epoch(model, train_loader, criterion, optimizer, device):\n    model.train()\n    running_loss = 0.0\n    correct = 0\n    total = 0\n    \n    for images, labels in train_loader:\n        images, labels = images.to(device), labels.to(device)\n        \n        optimizer.zero_grad()\n        outputs = model(images)\n        loss = criterion(outputs, labels)\n        \n        loss.backward()\n        optimizer.step()\n        \n        running_loss += loss.item()\n        _, predicted = outputs.max(1)\n        total += labels.size(0)\n        correct += predicted.eq(labels).sum().item()\n    \n    return running_loss/len(train_loader), 100.*correct/total\n\n# Training loop\nnum_epochs = 10\nbest_val_acc = 0.0\n\nfor epoch in range(num_epochs):\n    train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)\n    val_loss, val_acc = validate(model, val_loader, criterion, device)\n    \n    # Log metrics\n    wandb.log({\n        \"epoch\": epoch + 1,\n        \"train_loss\": train_loss,\n        \"train_accuracy\": train_acc,\n        \"val_loss\": val_loss,\n        \"val_accuracy\": val_acc\n    })\n    \n    print(f'Epoch [{epoch+1}/{num_epochs}]')\n    print(f'Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%')\n    print(f'Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-19T12:49:15.804639Z","iopub.execute_input":"2025-02-19T12:49:15.804846Z","iopub.status.idle":"2025-02-19T12:58:14.180929Z","shell.execute_reply.started":"2025-02-19T12:49:15.804827Z","shell.execute_reply":"2025-02-19T12:58:14.180013Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.manifold import TSNE\nimport numpy as np\n\ndef extract_features_and_visualize(model, loader, device, dim=2):\n    model.eval()\n    features = []\n    labels = []\n    \n    with torch.no_grad():\n        for images, batch_labels in loader:\n            images = images.to(device)\n            batch_features = model.extract_features(images)\n            features.append(batch_features.cpu())\n            labels.append(batch_labels)\n    \n    features = torch.cat(features, 0).numpy()\n    labels = torch.cat(labels, 0).numpy()\n    \n    # Apply t-SNE\n    tsne = TSNE(n_components=dim, random_state=42)\n    features_tsne = tsne.fit_transform(features)\n    \n    # Plot\n    if dim == 2:\n        plt.figure(figsize=(10, 8))\n        scatter = plt.scatter(features_tsne[:, 0], features_tsne[:, 1], \n                            c=labels, cmap='tab10')\n        plt.colorbar(scatter)\n        plt.title('t-SNE visualization of features (2D)')\n        wandb.log({\"TSNE_2D\": wandb.Image(plt)})\n    else:\n        fig = plt.figure(figsize=(10, 8))\n        ax = fig.add_subplot(111, projection='3d')\n        scatter = ax.scatter(features_tsne[:, 0], features_tsne[:, 1], \n                           features_tsne[:, 2], c=labels, cmap='tab10')\n        plt.colorbar(scatter)\n        plt.title('t-SNE visualization of features (3D)')\n        wandb.log({\"TSNE_3D\": wandb.Image(plt)})\n\n# Generate both 2D and 3D t-SNE plots\nextract_features_and_visualize(model, val_loader, device, dim=2)\nextract_features_and_visualize(model, val_loader, device, dim=3)\n\n# Close wandb run\nwandb.finish()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-19T12:58:14.181990Z","iopub.execute_input":"2025-02-19T12:58:14.182252Z","iopub.status.idle":"2025-02-19T12:59:20.785291Z","shell.execute_reply.started":"2025-02-19T12:58:14.182227Z","shell.execute_reply":"2025-02-19T12:59:20.784331Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from torchvision import transforms\nimport matplotlib.pyplot as plt\nfrom PIL import Image\n\nwandb.init(project=WANDB_PROJECT, name=\"augmented-data\")\n\n# Define augmentation transforms\naugmentation_transform = transforms.Compose([\n    transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),  # Random crop with size variation\n    transforms.RandomHorizontalFlip(p=0.5),  # Horizontal flip\n    transforms.RandomRotation(15),  # Random rotation up to 15 degrees\n    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),  # Color augmentation\n    transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),  # Random translation\n    transforms.ToTensor()\n])\n\n# Visualize augmentations\ndef visualize_augmentations(image_path, num_augmentations=5):\n    # Load original image\n    original_image = Image.open(image_path).convert('RGB')\n    \n    # Create subplot\n    plt.figure(figsize=(15, 3))\n    \n    # Show original image\n    plt.subplot(1, num_augmentations, 1)\n    plt.imshow(original_image)\n    plt.title('Original')\n    plt.axis('off')\n    \n    # Show augmented images\n    for i in range(num_augmentations-1):\n        augmented_image = augmentation_transform(original_image)\n        plt.subplot(1, num_augmentations, i+2)\n        plt.imshow(augmented_image.permute(1, 2, 0))\n        plt.title(f'Augmented {i+1}')\n        plt.axis('off')\n    \n    plt.tight_layout()\n    wandb.log({\"Augmentation Examples\": wandb.Image(plt)})\n    plt.show()\n\n# Visualize augmentations for a sample image\nsample_image_path = train_df['image_path'].iloc[0]  # Get first image path\nvisualize_augmentations(sample_image_path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-19T12:59:20.786237Z","iopub.execute_input":"2025-02-19T12:59:20.786489Z","iopub.status.idle":"2025-02-19T12:59:28.422579Z","shell.execute_reply.started":"2025-02-19T12:59:20.786468Z","shell.execute_reply":"2025-02-19T12:59:28.421717Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Dataset with augmentation\nclass AugmentedImageDataset(Dataset):\n    def __init__(self, dataframe, transform=None):\n        self.dataframe = dataframe.reset_index(drop=True)\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.dataframe)\n\n    def __getitem__(self, idx):\n        row = self.dataframe.iloc[idx]\n        image_path = row['image_path']\n        label = row['label']\n        \n        image = Image.open(image_path).convert(\"RGB\")\n        if self.transform:\n            image = self.transform(image)\n            \n        return image, label\n\n# Create augmented datasets and dataloaders\ntrain_dataset_aug = AugmentedImageDataset(train_df, transform=augmentation_transform)\nval_dataset = AugmentedImageDataset(val_df, transform=transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor()\n]))\n\ntrain_loader_aug = DataLoader(train_dataset_aug, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\nval_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-19T12:59:28.423674Z","iopub.execute_input":"2025-02-19T12:59:28.423978Z","iopub.status.idle":"2025-02-19T12:59:28.436338Z","shell.execute_reply.started":"2025-02-19T12:59:28.423947Z","shell.execute_reply":"2025-02-19T12:59:28.435319Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Initialize new model, criterion, and optimizer\nmodel_aug = FineTunedResNet18(num_classes=10).to(device)\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model_aug.parameters())\n\n# Initialize wandb\nwandb.init(project=WANDB_PROJECT, name=\"resnet18-augmented\")\nwandb.config.update({\n    \"model\": \"ResNet18 with Augmentation\",\n    \"epochs\": 10,\n    \"batch_size\": BATCH_SIZE,\n    \"learning_rate\": optimizer.param_groups[0]['lr']\n})\n\n# Training loop\nnum_epochs = 10\nbest_val_acc = 0.0\n\nfor epoch in range(num_epochs):\n    train_loss, train_acc = train_epoch(model_aug, train_loader_aug, criterion, optimizer, device)\n    val_loss, val_acc = validate(model_aug, val_loader, criterion, device)\n    \n    # Log metrics\n    wandb.log({\n        \"epoch\": epoch + 1,\n        \"train_loss\": train_loss,\n        \"train_accuracy\": train_acc,\n        \"val_loss\": val_loss,\n        \"val_accuracy\": val_acc\n    })\n    \n    print(f'Epoch [{epoch+1}/{num_epochs}]')\n    print(f'Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%')\n    print(f'Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%')\n\n# Evaluate final model\naccuracy, f1 = evaluate_model(model_aug, val_loader, device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-19T12:59:28.437282Z","iopub.execute_input":"2025-02-19T12:59:28.437581Z","execution_failed":"2025-02-19T13:05:00.581Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":" ","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}